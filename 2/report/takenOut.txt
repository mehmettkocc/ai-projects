Gradient ascent (GA) is one method that is used to find the extremum in the logistic function. We can tailor the derivative equation for maximizing log conditional likelihood (LCL) in the following way: $$LCL = \sum_{i=1}^{n} log p(y_i |x_i;w_i) $$ Note that this is computed for all positions in a sentence and all examples in the dataset. Then, the maximization process involves the derivative:$$\pder{LCL}{w_j} = \pder{logp(y|x;w)}{w_j}$$$$\pder{LCL}{w_j} = F_j(x,y) - \sum_{y'} F_j(x,y')p(y'|x;w) = F_j(x,y) - E_{y'~p(y'|x;w)}[F_j(x,y')]$$The time complexity of one update of the rule above is O(nd) where d is the cardinality of label space and n is the number of training examples. Since the cardinality of label space is $O(m^2)$ the complexity becomes $O(m^2n)$. The space complexity is $O(m^2)$ since x and y are already allocated inputs. \\ Stochastic gradient ascent (SGA) is employed to find out the assignment of $w_j$ at each step. The training data is sorted randomly and one example at a time is used in the following stochastic update rule until convergence:$$w_j = w_j + \lambda(F_j(x,y) - E_{y'~p(y'|x;w)}[F_j(x,y')] - 2\mu j)$$Note that, in this equation, the term $2\mu j$ corresponds to regularization employed to prevent overfitting. Time complexity thus becomes $O(m^2nJ)$ and the space complexity is $O(m^2J)$. In addition, for learning rate and regularization to treat each feature similarly, it is desirable to normalize the features by z-scoring. Note that z-scoring is more robust to noisy values compared to [0, 1] normalization.$$ x_k \longleftarrow \frac{x_k - \mu_k}{\sigma_k}\, for\, k=1,2,...,J$$Note that the affine mapping above makes each feature of mean ($\mu_i$) 0 and variance ($\sigma_i^2$) 1. It is important to use the same $\mu_i$ and $\sigma_i$ while normalizing the test set. \\\\Best $\mu$ can be selected by creating a validation set in the training data independent of the actual training set (i.e. the part of training data used for training). The objective here is to choose $\mu$ that maximizes LCL on the validation set $\{x_i , y_i \}_{i=1}^{t}$ where $t < n$.